{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 28065,
     "status": "ok",
     "timestamp": 1743661771897,
     "user": {
      "displayName": "Kali Muthu",
      "userId": "13932889007039424279"
     },
     "user_tz": -330
    },
    "id": "y-c9g-wLSniE",
    "outputId": "1020b164-b49d-409a-fc21-f92467982b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Ask a question about the document (or type 'exit' to quit): quit\n",
      "\n",
      "ðŸ“Œ Answer:\n",
      "The provided text does not contain information related to quitting.\n",
      "\n",
      "ðŸ”¹ Ask a question about the document (or type 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF for PDF processing\n",
    "import faiss\n",
    "import numpy as np\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ðŸ”‘ Set up Gemini API Key\n",
    "GEMINI_API_KEY = \"\"\n",
    "# Replace with your API key\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# âœ… Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text\n",
    "\n",
    "# âœ… Function to split text into smaller chunks\n",
    "def split_text(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i : i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# âœ… Function to generate embeddings using Gemini API\n",
    "def get_gemini_embeddings(texts):\n",
    "    model = \"models/text-embedding-004\"  # Free embedding model\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = genai.embed_content(model=model, content=text, task_type=\"retrieval_document\")\n",
    "        embeddings.append(response[\"embedding\"])\n",
    "    return np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "# âœ… Function to create FAISS vector store\n",
    "def create_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]  # Get embedding dimension\n",
    "    index = faiss.IndexFlatL2(dim)  # L2 distance-based FAISS index\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# âœ… Function to query FAISS and get relevant chunks\n",
    "def query_faiss(index, text_chunks, query, top_k=3):\n",
    "    query_embedding = get_gemini_embeddings([query])  # Get query embedding\n",
    "    distances, indices = index.search(query_embedding, top_k)  # Search in FAISS\n",
    "\n",
    "    results = [text_chunks[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "# âœ… Function to generate a final answer using Gemini\n",
    "def generate_answer(query, retrieved_text):\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant. The user asked the question:\n",
    "    \"{query}\"\n",
    "\n",
    "    Below is the most relevant information extracted from a document:\n",
    "    {retrieved_text}\n",
    "\n",
    "    Please provide a **concise and direct answer** based only on the given text.\n",
    "    \"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro\")  # âœ… Correct way to call Gemini\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    return response.text.strip()\n",
    "\n",
    "# âœ… Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/content/constitution_of_india_summary.pdf\"# ðŸ”¹ Replace with your PDF file path\n",
    "    text = extract_text_from_pdf(pdf_path)  # Extract text\n",
    "    text_chunks = split_text(text)  # Split into chunks\n",
    "    embeddings = get_gemini_embeddings(text_chunks)  # Generate embeddings\n",
    "\n",
    "    index = create_faiss_index(embeddings)  # Create FAISS index\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nðŸ”¹ Ask a question about the document (or type 'exit' to quit): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        retrieved_text = \" \".join(query_faiss(index, text_chunks, query))  # Combine retrieved chunks\n",
    "        answer = generate_answer(query, retrieved_text)  # Process with Gemini\n",
    "\n",
    "        print(\"\\nðŸ“Œ Answer:\")\n",
    "        print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gG_e7C9VbOf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNmpSPvAOY7ob7kLHE8csjP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
