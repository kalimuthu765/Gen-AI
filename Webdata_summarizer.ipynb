{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdKtyCqS4PY1c3K5xgeNkf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gosaitos/GEN-AI/blob/main/Webdata_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n",
        "!pip install chromadb\n"
      ],
      "metadata": {
        "id": "D5OKuIUvmu2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACZu2IWWDgQZ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # Switched to free embeddings\n",
        "import os\n",
        "\n",
        "# Define available sources\n",
        "AVAILABLE_SOURCES = {\n",
        "    \"wikipedia\": \"https://en.wikipedia.org/wiki/\",\n",
        "    \"arxiv\": \"https://arxiv.org/search/?query=\",\n",
        "    \"bbc_news\": \"https://www.bbc.co.uk/search?q=\",\n",
        "    \"nature\": \"https://www.nature.com/search?q=\",\n",
        "    \"custom\": \"\"  # For user-defined URLs\n",
        "}\n",
        "\n",
        "# Fetch content from Wikipedia\n",
        "def fetch_wikipedia(query):\n",
        "    url = AVAILABLE_SOURCES[\"wikipedia\"] + query.replace(\" \", \"_\")\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\", limit=5)\n",
        "        return \"\\n\".join([p.get_text() for p in paragraphs]) or \"No relevant content found.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching Wikipedia content: {e}\"\n",
        "\n",
        "# Fetch research content from ArXiv or Nature\n",
        "def fetch_research_papers(query, source):\n",
        "    if source == \"arxiv\":\n",
        "        url = AVAILABLE_SOURCES[\"arxiv\"] + query.replace(\" \", \"+\")\n",
        "    elif source == \"nature\":\n",
        "        url = AVAILABLE_SOURCES[\"nature\"] + query.replace(\" \", \"+\")\n",
        "    else:\n",
        "        return \"Invalid research source.\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        if source == \"arxiv\":\n",
        "            abstracts = soup.select(\"span.abstract-full\", limit=5)\n",
        "            return \"\\n\".join([a.get_text().strip() for a in abstracts]) or \"No relevant content found.\"\n",
        "        else:  # Nature\n",
        "            paragraphs = soup.find_all(\"p\", limit=5)\n",
        "            return \"\\n\".join([p.get_text() for p in paragraphs]) or \"No relevant content found.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching content from {source}: {e}\"\n",
        "\n",
        "# Fetch content from a custom URL\n",
        "def fetch_custom(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\", limit=5)\n",
        "        return \"\\n\".join([p.get_text() for p in paragraphs]) or \"No relevant content found.\"\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching content from {url}: {e}\"\n",
        "\n",
        "# Initialize LLM with OpenRouter, adding required headers\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.getenv(\"OPENROUTER_API_KEY\", \"\"),\n",
        "    model=\"mistralai/mixtral-8x7b-instruct\",  # Replace with correct model from OpenRouter\n",
        "    temperature=0.5,\n",
        "    default_headers={\n",
        "        \"HTTP-Referer\": \"http://localhost\",  # Required by OpenRouter\n",
        "        \"X-Title\": \"RAG Chatbot\"  # Required by OpenRouter\n",
        "    }\n",
        ")\n",
        "\n",
        "# Initialize ChromaDB with free Hugging Face embeddings (no OpenAI key needed)\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma(collection_name=\"dynamic_data\", embedding_function=embedding_model)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "# Define prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"Based on the following information:\\n{context}\\nAnswer this question: {question}\"\n",
        ")\n",
        "\n",
        "# Set up RetrievalQA chain\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# Store content in vector database\n",
        "def store_content(query, content, source):\n",
        "    if \"Error\" not in content:\n",
        "        vectorstore.add_texts(\n",
        "            texts=[content],\n",
        "            metadatas=[{\"query\": query, \"source\": source}]\n",
        "        )\n",
        "\n",
        "# RAG Chatbot function\n",
        "def rag_chatbot(query, source):\n",
        "    if source == \"wikipedia\":\n",
        "        content = fetch_wikipedia(query)\n",
        "    elif source in [\"arxiv\", \"nature\"]:\n",
        "        content = fetch_research_papers(query, source)\n",
        "    elif source.startswith(\"http\"):\n",
        "        content = fetch_custom(source)\n",
        "    else:\n",
        "        return \"Invalid source. Please select Wikipedia, ArXiv, Nature, or provide a valid URL.\"\n",
        "\n",
        "    store_content(query, content, source)\n",
        "    response = rag_chain({\"query\": query})\n",
        "    return response[\"result\"]\n",
        "\n",
        "# Test the chatbot\n",
        "query = \"Madras institute of technology\"\n",
        "source = \"wikipedia\"\n",
        "response = rag_chatbot(query, source)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u6qe2-TUEFM3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}