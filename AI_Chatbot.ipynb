{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gosaitos/GEN-AI/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5birvNcp73-z"
   },
   "outputs": [],
   "source": [
    "#!pip install langchain openai openrouter dotenv\n",
    "#!pip install gradio openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1sQ_GsrHb4p",
    "outputId": "70aa7c8c-e0c0-4369-b48a-cf02f463b8bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Available Free Models:\n",
      "- allenai/molmo-7b-d:free\n",
      "- bytedance-research/ui-tars-72b:free\n",
      "- qwen/qwen2.5-vl-3b-instruct:free\n",
      "- google/gemini-2.5-pro-exp-03-25:free\n",
      "- qwen/qwen2.5-vl-32b-instruct:free\n",
      "- deepseek/deepseek-chat-v3-0324:free\n",
      "- featherless/qwerky-72b:free\n",
      "- mistralai/mistral-small-3.1-24b-instruct:free\n",
      "- open-r1/olympiccoder-7b:free\n",
      "- open-r1/olympiccoder-32b:free\n",
      "- google/gemma-3-1b-it:free\n",
      "- google/gemma-3-4b-it:free\n",
      "- google/gemma-3-12b-it:free\n",
      "- rekaai/reka-flash-3:free\n",
      "- google/gemma-3-27b-it:free\n",
      "- deepseek/deepseek-r1-zero:free\n",
      "- qwen/qwq-32b:free\n",
      "- moonshotai/moonlight-16b-a3b-instruct:free\n",
      "- nousresearch/deephermes-3-llama-3-8b-preview:free\n",
      "- cognitivecomputations/dolphin3.0-r1-mistral-24b:free\n",
      "- cognitivecomputations/dolphin3.0-mistral-24b:free\n",
      "- google/gemini-2.0-flash-lite-preview-02-05:free\n",
      "- google/gemini-2.0-pro-exp-02-05:free\n",
      "- qwen/qwen2.5-vl-72b-instruct:free\n",
      "- mistralai/mistral-small-24b-instruct-2501:free\n",
      "- deepseek/deepseek-r1-distill-qwen-32b:free\n",
      "- deepseek/deepseek-r1-distill-qwen-14b:free\n",
      "- deepseek/deepseek-r1-distill-llama-70b:free\n",
      "- google/gemini-2.0-flash-thinking-exp:free\n",
      "- deepseek/deepseek-r1:free\n",
      "- sophosympatheia/rogue-rose-103b-v0.2:free\n",
      "- deepseek/deepseek-chat:free\n",
      "- google/gemini-2.0-flash-thinking-exp-1219:free\n",
      "- google/gemini-2.0-flash-exp:free\n",
      "- meta-llama/llama-3.3-70b-instruct:free\n",
      "- qwen/qwq-32b-preview:free\n",
      "- google/learnlm-1.5-pro-experimental:free\n",
      "- qwen/qwen-2.5-coder-32b-instruct:free\n",
      "- nvidia/llama-3.1-nemotron-70b-instruct:free\n",
      "- meta-llama/llama-3.2-1b-instruct:free\n",
      "- meta-llama/llama-3.2-11b-vision-instruct:free\n",
      "- meta-llama/llama-3.2-3b-instruct:free\n",
      "- qwen/qwen-2.5-72b-instruct:free\n",
      "- qwen/qwen-2.5-vl-7b-instruct:free\n",
      "- meta-llama/llama-3.1-8b-instruct:free\n",
      "- mistralai/mistral-nemo:free\n",
      "- qwen/qwen-2-7b-instruct:free\n",
      "- google/gemma-2-9b-it:free\n",
      "- mistralai/mistral-7b-instruct:free\n",
      "- microsoft/phi-3-mini-128k-instruct:free\n",
      "- microsoft/phi-3-medium-128k-instruct:free\n",
      "- meta-llama/llama-3-8b-instruct:free\n",
      "- openchat/openchat-7b:free\n",
      "- undi95/toppy-m-7b:free\n",
      "- huggingfaceh4/zephyr-7b-beta:free\n",
      "- gryphe/mythomax-l2-13b:free\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "response = requests.get(\"https://openrouter.ai/api/v1/models\", headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    free_models = [model[\"id\"] for model in response.json()[\"data\"] if \":free\" in model[\"id\"]]\n",
    "\n",
    "    if free_models:\n",
    "        print(\"‚úÖ Available Free Models:\")\n",
    "        for model in free_models:\n",
    "            print(\"-\", model)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No free models found.\")\n",
    "else:\n",
    "    print(\"‚ùå Error:\", response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtR8vX788nv6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your OpenRouter API key\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "1aKuLVKzF4Xj",
    "outputId": "1693c324-7cf8-43fe-d927-18f6186a6d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://933c9f4edacbb6e5f7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://933c9f4edacbb6e5f7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Ensure OpenRouter API key is set\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Missing OpenRouter API Key! Set it using os.environ.\")\n",
    "\n",
    "# Set OpenAI-compatible API details\n",
    "openai.api_key = OPENROUTER_API_KEY\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# Pick a valid OpenRouter model\n",
    "model = \"deepseek/deepseek-r1-distill-qwen-32b:free\"\n",
    "\n",
    "# Define chat history\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}]\n",
    "\n",
    "# Define function to interact with chatbot\n",
    "def chat(user_input):\n",
    "    global messages\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        ai_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "        return ai_response\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {e}\"\n",
    "\n",
    "# Create Gradio UI\n",
    "iface = gr.Interface(\n",
    "    fn=chat,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Chatbot using OpenRouter API\",\n",
    "    description=\"Ask me anything!\",\n",
    ")\n",
    "\n",
    "# Launch the chatbot\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGQrjUWV961v",
    "outputId": "3d4545e6-c36a-4715-b114-c0e1fadf4b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hello\n",
      "Chatbot: Hello! How can I assist you today?\n",
      "User: bye\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Ensure OpenRouter API key is set\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Missing OpenRouter API Key! Set it using os.environ.\")\n",
    "\n",
    "# Set OpenAI-compatible API details\n",
    "openai.api_key = OPENROUTER_API_KEY\n",
    "openai.api_base = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "# Define conversation history\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \").strip()\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    try:\n",
    "        # OpenAI-compatible API call\n",
    "        response = openai.ChatCompletion.create(\n",
    "            #model=\"meta-llama/llama-3-8b-instruct:free\",\n",
    "            model = \"deepseek/deepseek-r1-distill-qwen-32b:free\",# Change model if needed\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        # Print full response for debugging\n",
    "        #print(\"üîç FULL RESPONSE:\", response)\n",
    "\n",
    "        # Extract AI response safely\n",
    "        if \"choices\" in response and response[\"choices\"]:\n",
    "            ai_response = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(\"Chatbot:\", ai_response)\n",
    "\n",
    "            # Append AI response to chat history\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No 'choices' found in response. Check the API response format.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyML5B+vjAaHLgXWD9z9ZRmu",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
